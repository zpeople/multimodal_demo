{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0aeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "try:\n",
    "    get_ipython\n",
    "    current_dir = os.getcwd()\n",
    "except NameError:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Set pathï¼Œtemporary path expansion\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4334e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b90662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LLM_NAME =\"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "VISION_NAME = \"google/siglip-base-patch16-256\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a54f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LLM_SigLIP_Multimodal(nn.Module):\n",
    "    def __init__(self, llm_model_name=LLM_NAME, \n",
    "                 siglip_model_name=VISION_NAME):\n",
    "        super().__init__()\n",
    "        local_path = os.path.join(\"/root/autodl-tmp\",\"model/\")\n",
    "        print(local_path)\n",
    "        \n",
    "        # åŠ è½½LLMæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(local_path+llm_model_name)\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            local_path+llm_model_name,\n",
    "            device_map=device\n",
    "        )\n",
    "        self.device = next(self.llm.parameters()).device\n",
    "        print(f\"ä½¿ç”¨è®¾å¤‡: {self.device}\")\n",
    "        \n",
    "        # åŠ è½½SigLIPå›¾åƒç¼–ç å™¨\n",
    "        self.image_processor = AutoProcessor.from_pretrained(local_path+siglip_model_name)\n",
    "        self.image_encoder = AutoModel.from_pretrained(local_path+siglip_model_name)\n",
    "        self.image_encoder = self.image_encoder.to(self.device)\n",
    "        \n",
    "        # ç¡®ä¿åˆ†è¯å™¨æœ‰pad_token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # æŠ•å½±å±‚ï¼šå°†SigLIPçš„å›¾åƒç‰¹å¾æ˜ å°„åˆ°LLMçš„åµŒå…¥ç©ºé—´\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.image_encoder.config.vision_config.hidden_size, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, self.llm.config.hidden_size)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # æ·»åŠ å›¾åƒæ ‡è®°\n",
    "        self.image_token = \"<image>\"\n",
    "        self.tokenizer.add_tokens([self.image_token])\n",
    "        self.llm.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.llm = self.llm.to(self.device) \n",
    "    \n",
    "        \n",
    "    def encode_image(self, image):\n",
    "        \"\"\"ä½¿ç”¨SigLIPç¼–ç å›¾åƒå¹¶æŠ•å½±åˆ°LLMç©ºé—´\"\"\"\n",
    "        # print(image)\n",
    "        # å¤„ç†å›¾åƒ  æå–å›¾åƒç‰¹å¾ æ˜¾å¼æŒ‡å®š text=None\n",
    "        inputs = self.image_processor(text=None,images=image, return_tensors=\"pt\").to(self.device)\n",
    "        # print(inputs)\n",
    "        # è·å–å›¾åƒç‰¹å¾\n",
    "        with torch.no_grad():\n",
    "            vision_outputs = self.image_encoder.vision_model(**inputs)\n",
    "            # shape: [batch_size, num_patches + 1, vision_embed_dim]ï¼ˆåŒ…å«CLSæ ‡è®°ï¼‰\n",
    "            print(vision_outputs.last_hidden_state.shape)\n",
    "            image_features = vision_outputs.last_hidden_state[:, 0, :]  # [batch_size, vision_embed_dim]\n",
    "        \n",
    "        # æŠ•å½±åˆ°LLMçš„åµŒå…¥ç©ºé—´\n",
    "        projected_features = self.projection(image_features)\n",
    "        return projected_features\n",
    "    \n",
    "    def generate(self, image, prompt, max_length=200, temperature=0.7):\n",
    "        \"\"\"æ ¹æ®å›¾åƒå’Œæç¤ºç”Ÿæˆå“åº”\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # ç¼–ç å›¾åƒ\n",
    "            image_embeds = self.encode_image(image)\n",
    "            \n",
    "            # å‡†å¤‡è¾“å…¥æ–‡æœ¬ï¼ŒåŒ…å«å›¾åƒæ ‡è®°\n",
    "            input_text = f\"{self.image_token}\\n{prompt}\"\n",
    "            \n",
    "            # ç¼–ç æ–‡æœ¬\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(self.device)\n",
    "            print(inputs.input_ids)\n",
    "            # æ„å»ºè¾“å…¥åµŒå…¥ï¼šå°†å›¾åƒæ ‡è®°æ›¿æ¢ä¸ºå®é™…å›¾åƒç‰¹å¾\n",
    "            input_embeds = self.llm.get_input_embeddings()(inputs.input_ids)\n",
    "            image_token_id = self.tokenizer.convert_tokens_to_ids(self.image_token)\n",
    "            \n",
    "            # æ‰¾åˆ°å›¾åƒæ ‡è®°çš„ä½ç½®å¹¶æ›¿æ¢\n",
    "            for batch_idx in range(input_embeds.shape[0]):\n",
    "                image_positions = (inputs.input_ids[batch_idx] == image_token_id).nonzero()\n",
    "                if len(image_positions) > 0:\n",
    "                    pos = image_positions[0, -1]  # è·å–ç¬¬ä¸€ä¸ªå›¾åƒæ ‡è®°çš„ä½ç½®\n",
    "                    input_embeds[batch_idx, pos] = image_embeds[batch_idx]\n",
    "            \n",
    "            # ç”Ÿæˆå“åº”\n",
    "            outputs = self.llm.generate(\n",
    "                input_ids=inputs.input_ids, \n",
    "                inputs_embeds=input_embeds,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # è§£ç å¹¶è¿”å›ç»“æœ\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58474918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå§‹åŒ–å¤šæ¨¡æ€æ¨¡å‹...\n",
      "/root/autodl-tmp/model/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f123268c3bdd417e8733a615726dad32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨è®¾å¤‡: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"åˆå§‹åŒ–å¤šæ¨¡æ€æ¨¡å‹...\")\n",
    "model = LLM_SigLIP_Multimodal(\n",
    "    llm_model_name=LLM_NAME,\n",
    "    siglip_model_name=VISION_NAME\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8245843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”Ÿæˆå“åº”ä¸­...\n",
      "torch.Size([1, 256, 768])\n",
      "tensor([[151669,    198, 100700,  53481, 108893,  45930, 104597,   3837, 100630,\n",
      "         102122,   5373, 109840,  33108,  87267,   9370, 104556,   1773]],\n",
      "       device='cuda:0')\n",
      "\n",
      "===== æ¨¡å‹å“åº” =====\n",
      "<image>\n",
      "è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰©ä½“å’Œå¯èƒ½çš„æ°›å›´ã€‚ ç”±äºæˆ‘ç›®å‰æ— æ³•ç›´æ¥æŸ¥çœ‹æˆ–åˆ†æå›¾ç‰‡ï¼Œå› æ­¤æ— æ³•æä¾›å¯¹å…·ä½“å›¾ç‰‡å†…å®¹çš„è¯¦ç»†æè¿°ã€‚å¦‚æœä½ èƒ½æä¾›å›¾ç‰‡çš„è¯¦ç»†ä¿¡æ¯ï¼Œä¾‹å¦‚åœºæ™¯æè¿°ã€ç‰©ä½“ç‰¹å¾ã€é¢œè‰²æ­é…ã€æ„å›¾æ–¹å¼æˆ–æ°›å›´æ„Ÿå—ç­‰ï¼Œæˆ‘å¯ä»¥å¸®åŠ©ä½ è¿›è¡Œè¯¦ç»†çš„åˆ†æå’Œæè¿°ã€‚\n",
      "\n",
      "ä¾‹å¦‚ï¼Œä½ å¯ä»¥å‘Šè¯‰æˆ‘ï¼š\n",
      "- åœºæ™¯æ˜¯å®¤å†…è¿˜æ˜¯å®¤å¤–ï¼Ÿæ˜¯åŸå¸‚ã€ä¹¡æ‘ã€æ£®æ—ã€æ²™æ¼ è¿˜æ˜¯å…¶ä»–ï¼Ÿ\n",
      "- å›¾ç‰‡ä¸­æœ‰å“ªäº›ä¸»è¦ç‰©ä½“ï¼Ÿæ¯”å¦‚äººç‰©ã€å»ºç­‘ã€æ¤ç‰©ã€è½¦è¾†ã€åŠ¨ç‰©ç­‰ï¼Ÿ\n",
      "- é¢œè‰²å’Œå…‰çº¿å¦‚ä½•ï¼Ÿæ˜¯æ˜äº®æ¸©æš–çš„é˜³å…‰ï¼Œè¿˜æ˜¯é˜´æš—å†·è‰²è°ƒçš„é»„æ˜ï¼Ÿ\n",
      "- æ°›å›´æ˜¯å®é™ã€ç´§å¼ ã€æ¬¢ä¹ã€æ‚²ä¼¤ã€ç¥ç§˜è¿˜æ˜¯å…¶ä»–æƒ…ç»ªï¼Ÿ\n",
      "- æ˜¯å¦æœ‰ç‰¹å®šçš„é£æ ¼æˆ–è‰ºæœ¯è¡¨ç°æ‰‹æ³•ï¼Ÿæ¯”å¦‚å†™å®ã€æŠ½è±¡ã€è¶…ç°å®ã€æ‘„å½±ã€ç»˜ç”»ç­‰ï¼Ÿ\n",
      "\n",
      "æä¾›è¿™äº›ä¿¡æ¯åï¼Œæˆ‘å°†ä¸ºä½ è¯¦ç»†æè¿°å›¾ç‰‡çš„å†…å®¹ã€åœºæ™¯ã€ç‰©ä½“å’Œå¯èƒ½çš„æ°›å›´ã€‚æœŸå¾…ä½ çš„è¡¥å……ï¼ ğŸ“¸âœ¨\n",
      "\n",
      "å¦‚æœä½ æœ‰å›¾ç‰‡é“¾æ¥æˆ–å¯ä»¥æ–‡å­—æè¿°çš„å†…å®¹ï¼Œä¹Ÿæ¬¢è¿ç»§ç»­å‘é€ï¼\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # åŠ è½½ç¤ºä¾‹å›¾åƒ\n",
    "    image_path = \"example.jpg\"  # æ›¿æ¢ä¸ºä½ çš„å›¾åƒè·¯å¾„\n",
    "    # image = Image.open(image_path).convert(\"RGB\")\n",
    "    # print(f\"æˆåŠŸåŠ è½½å›¾åƒ: {image_path}\")\n",
    "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "  \n",
    "    # ç¤ºä¾‹æç¤º\n",
    "    prompt = \"è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰©ä½“å’Œå¯èƒ½çš„æ°›å›´ã€‚\"\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    print(\"ç”Ÿæˆå“åº”ä¸­...\")\n",
    "    response = model.generate(image, prompt, max_length=300)\n",
    "    \n",
    "    print(\"\\n===== æ¨¡å‹å“åº” =====\")\n",
    "    print(response)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"é”™è¯¯: æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶ {image_path}\")\n",
    "    print(\"è¯·ç¡®ä¿å›¾åƒæ–‡ä»¶å­˜åœ¨æˆ–ä¿®æ”¹å›¾åƒè·¯å¾„\")\n",
    "except Exception as e:\n",
    "    print(f\"å‘ç”Ÿé”™è¯¯: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74537e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
