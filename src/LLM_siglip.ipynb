{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0aeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "try:\n",
    "    get_ipython\n",
    "    current_dir = os.getcwd()\n",
    "except NameError:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Set path，temporary path expansion\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4334e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b90662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LLM_NAME =\"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "VISION_NAME = \"google/siglip-base-patch16-256\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a54f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LLM_SigLIP_Multimodal(nn.Module):\n",
    "    def __init__(self, llm_model_name=LLM_NAME, \n",
    "                 siglip_model_name=VISION_NAME):\n",
    "        super().__init__()\n",
    "        local_path = os.path.join(\"/root/autodl-tmp\",\"model/\")\n",
    "        print(local_path)\n",
    "        \n",
    "        # 加载LLM模型和分词器\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(local_path+llm_model_name)\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            local_path+llm_model_name,\n",
    "            device_map=device\n",
    "        )\n",
    "        self.device = next(self.llm.parameters()).device\n",
    "        print(f\"使用设备: {self.device}\")\n",
    "        \n",
    "        # 加载SigLIP图像编码器\n",
    "        self.image_processor = AutoProcessor.from_pretrained(local_path+siglip_model_name)\n",
    "        self.image_encoder = AutoModel.from_pretrained(local_path+siglip_model_name)\n",
    "        self.image_encoder = self.image_encoder.to(self.device)\n",
    "        \n",
    "        # 确保分词器有pad_token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # 投影层：将SigLIP的图像特征映射到LLM的嵌入空间\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.image_encoder.config.vision_config.hidden_size, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, self.llm.config.hidden_size)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # 添加图像标记\n",
    "        self.image_token = \"<image>\"\n",
    "        self.tokenizer.add_tokens([self.image_token])\n",
    "        self.llm.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.llm = self.llm.to(self.device) \n",
    "    \n",
    "        \n",
    "    def encode_image(self, image):\n",
    "        \"\"\"使用SigLIP编码图像并投影到LLM空间\"\"\"\n",
    "        # print(image)\n",
    "        # 处理图像  提取图像特征 显式指定 text=None\n",
    "        inputs = self.image_processor(text=None,images=image, return_tensors=\"pt\").to(self.device)\n",
    "        # print(inputs)\n",
    "        # 获取图像特征\n",
    "        with torch.no_grad():\n",
    "            vision_outputs = self.image_encoder.vision_model(**inputs)\n",
    "            # shape: [batch_size, num_patches + 1, vision_embed_dim]（包含CLS标记）\n",
    "            print(vision_outputs.last_hidden_state.shape)\n",
    "            image_features = vision_outputs.last_hidden_state[:, 0, :]  # [batch_size, vision_embed_dim]\n",
    "        \n",
    "        # 投影到LLM的嵌入空间\n",
    "        projected_features = self.projection(image_features)\n",
    "        return projected_features\n",
    "    \n",
    "    def generate(self, image, prompt, max_length=200, temperature=0.7):\n",
    "        \"\"\"根据图像和提示生成响应\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # 编码图像\n",
    "            image_embeds = self.encode_image(image)\n",
    "            \n",
    "            # 准备输入文本，包含图像标记\n",
    "            input_text = f\"{self.image_token}\\n{prompt}\"\n",
    "            \n",
    "            # 编码文本\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(self.device)\n",
    "            print(inputs.input_ids)\n",
    "            # 构建输入嵌入：将图像标记替换为实际图像特征\n",
    "            input_embeds = self.llm.get_input_embeddings()(inputs.input_ids)\n",
    "            image_token_id = self.tokenizer.convert_tokens_to_ids(self.image_token)\n",
    "            \n",
    "            # 找到图像标记的位置并替换\n",
    "            for batch_idx in range(input_embeds.shape[0]):\n",
    "                image_positions = (inputs.input_ids[batch_idx] == image_token_id).nonzero()\n",
    "                if len(image_positions) > 0:\n",
    "                    pos = image_positions[0, -1]  # 获取第一个图像标记的位置\n",
    "                    input_embeds[batch_idx, pos] = image_embeds[batch_idx]\n",
    "            \n",
    "            # 生成响应\n",
    "            outputs = self.llm.generate(\n",
    "                input_ids=inputs.input_ids, \n",
    "                inputs_embeds=input_embeds,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # 解码并返回结果\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58474918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化多模态模型...\n",
      "/root/autodl-tmp/model/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f123268c3bdd417e8733a615726dad32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"初始化多模态模型...\")\n",
    "model = LLM_SigLIP_Multimodal(\n",
    "    llm_model_name=LLM_NAME,\n",
    "    siglip_model_name=VISION_NAME\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8245843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成响应中...\n",
      "torch.Size([1, 256, 768])\n",
      "tensor([[151669,    198, 100700,  53481, 108893,  45930, 104597,   3837, 100630,\n",
      "         102122,   5373, 109840,  33108,  87267,   9370, 104556,   1773]],\n",
      "       device='cuda:0')\n",
      "\n",
      "===== 模型响应 =====\n",
      "<image>\n",
      "详细描述这张图片的内容，包括场景、物体和可能的氛围。 由于我目前无法直接查看或分析图片，因此无法提供对具体图片内容的详细描述。如果你能提供图片的详细信息，例如场景描述、物体特征、颜色搭配、构图方式或氛围感受等，我可以帮助你进行详细的分析和描述。\n",
      "\n",
      "例如，你可以告诉我：\n",
      "- 场景是室内还是室外？是城市、乡村、森林、沙漠还是其他？\n",
      "- 图片中有哪些主要物体？比如人物、建筑、植物、车辆、动物等？\n",
      "- 颜色和光线如何？是明亮温暖的阳光，还是阴暗冷色调的黄昏？\n",
      "- 氛围是宁静、紧张、欢乐、悲伤、神秘还是其他情绪？\n",
      "- 是否有特定的风格或艺术表现手法？比如写实、抽象、超现实、摄影、绘画等？\n",
      "\n",
      "提供这些信息后，我将为你详细描述图片的内容、场景、物体和可能的氛围。期待你的补充！ 📸✨\n",
      "\n",
      "如果你有图片链接或可以文字描述的内容，也欢迎继续发送！\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 加载示例图像\n",
    "    image_path = \"example.jpg\"  # 替换为你的图像路径\n",
    "    # image = Image.open(image_path).convert(\"RGB\")\n",
    "    # print(f\"成功加载图像: {image_path}\")\n",
    "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "  \n",
    "    # 示例提示\n",
    "    prompt = \"详细描述这张图片的内容，包括场景、物体和可能的氛围。\"\n",
    "    \n",
    "    # 生成响应\n",
    "    print(\"生成响应中...\")\n",
    "    response = model.generate(image, prompt, max_length=300)\n",
    "    \n",
    "    print(\"\\n===== 模型响应 =====\")\n",
    "    print(response)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"错误: 未找到图像文件 {image_path}\")\n",
    "    print(\"请确保图像文件存在或修改图像路径\")\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74537e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
